


# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf

#  Import and read the charity_data.csv.
import pandas as pd
application_df = pd.read_csv("https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv")
application_df = pd.DataFrame(application_df)
application_df


!pip install keras-tuner


# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
clean_application_df = application_df.drop(['EIN','NAME'], axis=1)


clean_application_df


# Determine the number of unique values in each column.
clean_application_df.nunique()





# For columns with more than 10 unique values ('APPLICATION_TYPE' and 'CLASSIFICATION'), determine the number of data points for each unique value
# Create a list of application types to be replaced. Replace rare categorical variables with 'Other'

# Step 1: Determine the number of data points for each unique value
application_type_counts = clean_application_df['APPLICATION_TYPE'].value_counts()

# Step 2: Create a list of application types to be replaced
# Define a threshold for what we consider as 'rare'
threshold = 10
application_types_to_replace = application_type_counts[application_type_counts < threshold].index.tolist()

# Step 3: Replace rare application types with 'Other' using .replace()
clean_application_df['APPLICATION_TYPE'] = clean_application_df['APPLICATION_TYPE'].replace(application_types_to_replace, "Other")

# Step 4: Check to make sure replacement was successful
print(clean_application_df['APPLICATION_TYPE'].value_counts())


# Do the same for 'Classification'

# Step 1: Determine the number of data points for each unique value
classification_counts = clean_application_df['CLASSIFICATION'].value_counts()

# Step 2: Create a list of classification types to be replaced
# Define a threshold for what we consider as 'rare'
threshold = 10
classifications_to_replace = classification_counts[classification_counts < threshold].index.tolist()

# Step 3: Replace rare classification types with 'Other' using .replace()
clean_application_df['CLASSIFICATION'] = clean_application_df['CLASSIFICATION'].replace(classifications_to_replace, "Other")

# Step 4: Check to make sure replacement was successful
print(clean_application_df['CLASSIFICATION'].value_counts())


# Convert categorical data to numeric with `pd.get_dummies`
cat_application_df = pd.get_dummies(clean_application_df[['APPLICATION_TYPE','AFFILIATION','CLASSIFICATION','USE_CASE',
                                      'ORGANIZATION', 'INCOME_AMT',	'SPECIAL_CONSIDERATIONS', 'STATUS', 'IS_SUCCESSFUL', 'ASK_AMT']])


cat_application_df


# Split our preprocessed data into our features and target arrays
# Remove "IS_SUCCESSFUL" target from features data
y = cat_application_df.IS_SUCCESSFUL.values
X = cat_application_df.drop(columns="IS_SUCCESSFUL").values

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)


# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)





# Define the model - deep neural net
nn_model = tf.keras.models.Sequential()

# Add first Dense layer, including the input layer (input_dim should match the number of features)
input_dim = X_train_scaled.shape[1]
nn_model.add(tf.keras.layers.Dense(units=32, activation="relu", input_dim=input_dim))

# Add additional hidden layers
nn_model.add(tf.keras.layers.Dense(units=24, activation="relu"))
nn_model.add(tf.keras.layers.Dense(units=8, activation="relu"))

# Add the output layer (assuming binary classification)
nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the Sequential model
nn_model.summary()

# Compile the model
nn_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train the model
fit_model = nn_model.fit(X_train_scaled, y_train, epochs=50)

# Evaluate the model
loss, accuracy = nn_model.evaluate(X_test_scaled, y_test)
print(f"Test Accuracy: {accuracy}")





# Define the model - deep neural net
nn_model = tf.keras.models.Sequential()

# Add first Dense layer, including the input layer (input_dim should match the number of features)
input_dim = X_train_scaled.shape[1]
nn_model.add(tf.keras.layers.Dense(units=32, activation="relu", input_dim=input_dim))

# Add additional hidden layers
nn_model.add(tf.keras.layers.Dense(units=24, activation="relu"))
nn_model.add(tf.keras.layers.Dense(units=8, activation="relu"))

# Add the output layer (assuming binary classification)
nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the Sequential model
nn_model.summary()

# Compile the model
nn_model.compile(loss="binary_crossentropy", optimizer="rmsprop", metrics=["accuracy"])

# Train the model
fit_model = nn_model.fit(X_train_scaled, y_train, epochs=40)





# Add layers with more neurons
input_dim = X_train_scaled.shape[1]
nn_model.add(tf.keras.layers.Dense(units=64, activation="relu", input_dim=input_dim))  # Increased neurons
nn_model.add(tf.keras.layers.Dense(units=32, activation="relu"))  # Increased neurons
nn_model.add(tf.keras.layers.Dense(units=16, activation="relu"))  # Increased neurons
nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Compile the model with RMSprop optimizer
nn_model.compile(loss="binary_crossentropy", optimizer="rmsprop", metrics=["accuracy"])

# Train the model
fit_model = nn_model.fit(X_train_scaled, y_train, epochs=100)

# Evaluate the model
loss, accuracy = nn_model.evaluate(X_test_scaled, y_test)
print(f"Test Accuracy: {accuracy}")





# Add layers with more neurons and additional techniques
input_dim = X_train_scaled.shape[1]
nn_model.add(tf.keras.layers.Dense(units=128, activation="relu", input_dim=input_dim))
nn_model.add(tf.keras.layers.Dropout(0.5))  # Add dropout layer
nn_model.add(tf.keras.layers.Dense(units=64, activation="relu"))
nn_model.add(tf.keras.layers.Dropout(0.5))  # Add dropout layer
nn_model.add(tf.keras.layers.Dense(units=32, activation="relu"))
nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Compile the model with RMSprop optimizer and different learning rate
optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)
nn_model.compile(loss="binary_crossentropy", optimizer=optimizer, metrics=["accuracy"])

# Train the model with different batch size and epochs
fit_model = nn_model.fit(X_train_scaled, y_train, epochs=200, batch_size=64, validation_split=0.2)

# Evaluate the model
loss, accuracy = nn_model.evaluate(X_test_scaled, y_test)
print(f"Test Accuracy: {accuracy}")


# Save the model to HDF5 file
nn_model.save("AlphabetSoupCharity_Optimization_RMSprop_Improved.h5")
